[37m[36mINFO[0m[0m 07/05 06:55:38 | Command :: train_all.py exp_name --dataset PACS --data_dir /content/drive/MyDrive/code/miro/my/datasets/path --algorithm HYPERNET
Environment:
	Python: 3.10.12
	PyTorch: 2.3.0+cu121
	Torchvision: 0.18.0+cu121
	CUDA: 12.1
	CUDNN: 8906
	NumPy: 1.25.2
	PIL: 9.4.0
Args:
	algorithm: HYPERNET
	checkpoint_freq: None
	configs: []
	data_dir: /content/drive/MyDrive/code/miro/my/datasets/path
	dataset: PACS
	debug: False
	deterministic: True
	evalmode: fast
	holdout_fraction: 0.2
	model_save: None
	name: exp_name
	out_dir: train_output/PACS/240705_06-55-38_exp_name
	out_root: train_output/PACS
	prebuild_loader: False
	seed: 0
	show: False
	steps: None
	tb_freq: 10
	test_envs: None
	trial_seed: 0
	unique_name: 240705_06-55-38_exp_name
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: True
	pretrained: True
	lr: 5e-05
	batch_size: 16
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 32
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
Dataset:
	[PACS] #envs=4, #classes=7
	env0: art_painting (#2048)
	env1: cartoon (#2344)
	env2: photo (#1670)
	env3: sketch (#3929)

[37m[36mINFO[0m[0m 07/05 06:55:38 | n_steps = 5001
[37m[36mINFO[0m[0m 07/05 06:55:38 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 07/05 06:55:38 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 07/05 06:55:38 | Target test envs = [[0], [1], [2], [3]]
[37m[36mINFO[0m[0m 07/05 06:55:38 | 
[37m[36mINFO[0m[0m 07/05 06:55:38 | Testenv name escaping te_art_painting -> te_art_painting
[37m[36mINFO[0m[0m 07/05 06:55:38 | Test envs = [0], name = te_art_painting
[37m[36mINFO[0m[0m 07/05 06:55:38 | Batch sizes for each domain: [0, 16, 16, 16] (total=48)
[37m[36mINFO[0m[0m 07/05 06:55:38 | steps-per-epoch for each domain: 117.25, 83.50, 196.50 -> min = 83.50
[37m[36mINFO[0m[0m 07/05 06:55:42 | # of params = 11689512
[37m[36mINFO[0m[0m 07/05 07:00:56 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_out    env2_out    env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 07/05 07:00:56 | 0.000000    0.002445    0.000000    0.000000    11.929807   0.000000    0.002445    0.000000    0.000000    0.000000    0           0.000000    11.036709   6.208760    307.592880 
[37m[36mINFO[0m[0m 07/05 07:05:53 | 0.623551    0.674817    0.000000    0.896863    0.333395    0.623551    0.674817    0.880342    0.949102    0.861146    200         2.395210    1.597660    1.369556    22.908475  
[37m[36mINFO[0m[0m 07/05 07:07:56 | 0.708969    0.757946    0.000000    0.927381    0.218410    0.708969    0.757946    0.920940    0.949102    0.912102    400         4.790419    0.231037    0.495832    23.990663  
[37m[36mINFO[0m[0m 07/05 07:09:52 | 0.748627    0.772616    0.000000    0.949405    0.174812    0.748627    0.772616    0.933761    0.973054    0.941401    600         7.185629    0.143302    0.455578    25.389394  
[37m[36mINFO[0m[0m 07/05 07:11:47 | 0.734594    0.731051    0.000000    0.948836    0.166935    0.734594    0.731051    0.942308    0.964072    0.940127    800         9.580838    0.093690    0.448723    24.494018  
[37m[36mINFO[0m[0m 07/05 07:13:41 | 0.707138    0.753056    0.000000    0.948851    0.169974    0.707138    0.753056    0.952991    0.961078    0.932484    1000        11.976048   0.072084    0.441782    26.188456  
[37m[36mINFO[0m[0m 07/05 07:15:36 | 0.715070    0.721271    0.000000    0.956804    0.152906    0.715070    0.721271    0.955128    0.970060    0.945223    1200        14.371257   0.055611    0.448095    24.905805  
[37m[36mINFO[0m[0m 07/05 07:17:30 | 0.743746    0.762836    0.000000    0.953542    0.172496    0.743746    0.762836    0.948718    0.973054    0.938854    1400        16.766467   0.049929    0.442992    25.995804  
[37m[36mINFO[0m[0m 07/05 07:19:25 | 0.748627    0.748166    0.000000    0.950419    0.162174    0.748627    0.748166    0.944444    0.973054    0.933758    1600        19.161677   0.035381    0.445816    25.287728  
[37m[36mINFO[0m[0m 07/05 07:21:17 | 0.779744    0.782396    0.000000    0.954381    0.167744    0.779744    0.782396    0.950855    0.967066    0.945223    1800        21.556886   0.035718    0.435056    25.291831  
[37m[36mINFO[0m[0m 07/05 07:23:10 | 0.749237    0.762836    0.000000    0.947695    0.178981    0.749237    0.762836    0.931624    0.970060    0.941401    2000        23.952096   0.032671    0.441720    23.936616  
[37m[36mINFO[0m[0m 07/05 07:25:03 | 0.778523    0.789731    0.000000    0.952833    0.169359    0.778523    0.789731    0.955128    0.967066    0.936306    2200        26.347305   0.028269    0.438190    25.767611  
[37m[36mINFO[0m[0m 07/05 07:26:58 | 0.774253    0.782396    0.000000    0.955093    0.174013    0.774253    0.782396    0.952991    0.967066    0.945223    2400        28.742515   0.022088    0.448346    25.558172  
[37m[36mINFO[0m[0m 07/05 07:28:51 | 0.751678    0.782396    0.000000    0.959236    0.178853    0.751678    0.782396    0.957265    0.979042    0.941401    2600        31.137725   0.026552    0.431807    26.295168  
[37m[36mINFO[0m[0m 07/05 07:30:45 | 0.752288    0.770171    0.000000    0.955221    0.179130    0.752288    0.770171    0.952991    0.961078    0.951592    2800        33.532934   0.023744    0.442460    25.359205  
[37m[36mINFO[0m[0m 07/05 07:32:38 | 0.752288    0.792176    0.000000    0.952111    0.167721    0.752288    0.792176    0.955128    0.961078    0.940127    3000        35.928144   0.021416    0.443737    24.639317  
[37m[36mINFO[0m[0m 07/05 07:34:34 | 0.777303    0.821516    0.000000    0.943299    0.193023    0.777303    0.821516    0.955128    0.946108    0.928662    3200        38.323353   0.016472    0.445950    26.023985  
[37m[36mINFO[0m[0m 07/05 07:36:27 | 0.761440    0.784841    0.000000    0.951665    0.173732    0.761440    0.784841    0.955128    0.952096    0.947771    3400        40.718563   0.016820    0.448105    23.560115  
[37m[36mINFO[0m[0m 07/05 07:38:22 | 0.801708    0.801956    0.000000    0.953847    0.205140    0.801708    0.801956    0.965812    0.967066    0.928662    3600        43.113772   0.022168    0.448673    25.928732  
[37m[36mINFO[0m[0m 07/05 07:40:16 | 0.765711    0.760391    0.000000    0.948107    0.179436    0.765711    0.760391    0.952991    0.946108    0.945223    3800        45.508982   0.023357    0.444171    24.625905  
[37m[36mINFO[0m[0m 07/05 07:42:10 | 0.790116    0.814181    0.000000    0.950540    0.193323    0.790116    0.814181    0.955128    0.955090    0.941401    4000        47.904192   0.018534    0.442208    25.893121  
[37m[36mINFO[0m[0m 07/05 07:44:03 | 0.752898    0.748166    0.000000    0.948681    0.215354    0.752898    0.748166    0.952991    0.949102    0.943949    4200        50.299401   0.019299    0.443643    24.163558  
[37m[36mINFO[0m[0m 07/05 07:45:57 | 0.732154    0.733496    0.000000    0.957797    0.203564    0.732154    0.733496    0.970085    0.958084    0.945223    4400        52.694611   0.011306    0.439999    26.018164  
[37m[36mINFO[0m[0m 07/05 07:47:53 | 0.749237    0.735941    0.000000    0.958230    0.170049    0.749237    0.735941    0.963675    0.967066    0.943949    4600        55.089820   0.014226    0.445588    26.240554  
[37m[36mINFO[0m[0m 07/05 07:49:45 | 0.743136    0.728606    0.000000    0.948693    0.190820    0.743136    0.728606    0.931624    0.973054    0.941401    4800        57.485030   0.013867    0.436684    25.133925  
[37m[36mINFO[0m[0m 07/05 07:51:39 | 0.779134    0.784841    0.000000    0.951673    0.190498    0.779134    0.784841    0.948718    0.961078    0.945223    5000        59.880240   0.016399    0.443716    25.331689  
[37m[36mINFO[0m[0m 07/05 07:51:39 | ---
[37m[36mINFO[0m[0m 07/05 07:51:39 | training-domain validation = 75.168%
[37m[36mINFO[0m[0m 07/05 07:51:40 | 
[37m[36mINFO[0m[0m 07/05 07:51:40 | Testenv name escaping te_cartoon -> te_cartoon
[37m[36mINFO[0m[0m 07/05 07:51:40 | Test envs = [1], name = te_cartoon
[37m[36mINFO[0m[0m 07/05 07:51:40 | Batch sizes for each domain: [16, 0, 16, 16] (total=48)
[37m[36mINFO[0m[0m 07/05 07:51:40 | steps-per-epoch for each domain: 102.44, 83.50, 196.50 -> min = 83.50
[37m[36mINFO[0m[0m 07/05 07:51:41 | # of params = 11689512
[37m[36mINFO[0m[0m 07/05 07:52:09 | test_in     test_out    train_in    train_out   tr_outloss  env0_out    env1_in     env1_out    env2_out    env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 07/05 07:52:09 | 0.000000    0.000000    0.000000    0.000815    11.502261   0.002445    0.000000    0.000000    0.000000    0.000000    0           0.000000    11.098114   1.170907    26.825766  
[37m[36mINFO[0m[0m 07/05 07:54:04 | 0.594883    0.568376    0.000000    0.890860    0.346548    0.889976    0.594883    0.568376    0.943114    0.839490    200         2.395210    1.653126    0.442849    26.052563  
[37m[36mINFO[0m[0m 07/05 07:56:00 | 0.652985    0.626068    0.000000    0.930925    0.221399    0.921760    0.652985    0.626068    0.955090    0.915924    400         4.790419    0.230058    0.450028    26.279598  
[37m[36mINFO[0m[0m 07/05 07:57:55 | 0.696695    0.670940    0.000000    0.937420    0.215661    0.933985    0.696695    0.670940    0.961078    0.917197    600         7.185629    0.135051    0.446596    24.978075  
[37m[36mINFO[0m[0m 07/05 07:59:49 | 0.717484    0.694444    0.000000    0.939426    0.194451    0.924205    0.717484    0.694444    0.973054    0.921019    800         9.580838    0.095726    0.441865    26.156137  
[37m[36mINFO[0m[0m 07/05 08:01:47 | 0.721215    0.702991    0.000000    0.952923    0.151187    0.941320    0.721215    0.702991    0.976048    0.941401    1000        11.976048   0.074630    0.455620    26.181747  
[37m[36mINFO[0m[0m 07/05 08:03:43 | 0.749467    0.728632    0.000000    0.951169    0.155534    0.943765    0.749467    0.728632    0.967066    0.942675    1200        14.371257   0.059656    0.450314    25.858040  
